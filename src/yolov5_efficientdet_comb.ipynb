{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac50860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, cv2, torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# YOLOv5\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# EfficientDet\n",
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchPredict\n",
    "\n",
    "# COCO API\n",
    "try:\n",
    "    from pycocotools.coco import COCO\n",
    "    from pycocotools.cocoeval import COCOeval\n",
    "    COCO_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"pycocotools ÏóÜÏùå\")\n",
    "    COCO_AVAILABLE = False\n",
    "\n",
    "import matplotlib.font_manager as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edba3f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "BASE_DIR = Path.cwd().parent\n",
    "IMG_DIR = BASE_DIR / \"data/raw/images\"\n",
    "JSON_DIR = BASE_DIR / \"data/raw/json_labels\"\n",
    "DATASET_YOLO = BASE_DIR / \"processed/preprocessed_data/yolov5\"\n",
    "DATASET_EFFDET = BASE_DIR / \"processed/preprocessed_data/efficientdet\"\n",
    "RESULT_DIR = BASE_DIR / \"processed/results_comparison\"\n",
    "YOLO_WEIGHTS_FILE = RESULT_DIR / \"yolov5su.pt\"\n",
    "\n",
    "for d in [DATASET_YOLO, DATASET_EFFDET, RESULT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02bc9ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÌïúÍ∏Ä Ìè∞Ìä∏ ÏÑ§Ï†ï ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "# ÌïúÍµ≠Ïñ¥ Ìè∞Ìä∏\n",
    "def setup_korean_font():\n",
    "    try:\n",
    "        # Windows\n",
    "        if os.name == 'nt':\n",
    "            font_path = 'C:/Windows/Fonts/malgun.ttf'\n",
    "            if os.path.exists(font_path):\n",
    "                font_name = fm.FontProperties(fname=font_path).get_name()\n",
    "                plt.rc('font', family=font_name)\n",
    "            else:\n",
    "                plt.rc('font', family='DejaVu Sans')\n",
    "        # Mac\n",
    "        elif os.name == 'posix':\n",
    "            plt.rc('font', family='AppleGothic')\n",
    "        \n",
    "        plt.rcParams['axes.unicode_minus'] = False\n",
    "        print(\"ÌïúÍ∏Ä Ìè∞Ìä∏ ÏÑ§Ï†ï ÏôÑÎ£å\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ìè∞Ìä∏ ÏÑ§Ï†ï Ïã§Ìå®: {e}\")\n",
    "        plt.rc('font', family='DejaVu Sans')\n",
    "\n",
    "setup_korean_font()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cb15eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨\n",
    "def preprocess_data():\n",
    "    jsons = list(JSON_DIR.glob(\"*.json\"))\n",
    "    if not jsons:\n",
    "        return {}, []\n",
    "    \n",
    "    train, temp = train_test_split(jsons, train_size=0.8, random_state=42)\n",
    "    val, test = train_test_split(temp, train_size=0.5, random_state=42)\n",
    "    splits = {'train': [], 'val': [], 'test': []}\n",
    "    classes, class_to_idx = [], {}\n",
    "    \n",
    "    for split, files in zip(['train', 'val', 'test'], [train, val, test]):\n",
    "        for j in tqdm(files, desc=f\"Loading {split}\"):\n",
    "            with open(j, 'r', encoding='utf-8') as f:\n",
    "                d = json.load(f)\n",
    "\n",
    "            img_path = None\n",
    "            for ext in ['.jpg', '.png', '.jpeg', '.JPG', '.PNG', '.JPEG']:\n",
    "                p = IMG_DIR / f\"{j.stem}{ext}\"\n",
    "                if p.exists():\n",
    "                    img_path = str(p)\n",
    "                    break\n",
    "            if not img_path:\n",
    "                continue\n",
    "            \n",
    "            name = f\"{d['cate1']}_{d['cate3']}\"\n",
    "            if name not in classes:\n",
    "                class_to_idx[name] = len(classes)\n",
    "                classes.append(name)\n",
    "            \n",
    "            bbox = d['bndbox']\n",
    "            splits[split].append({\n",
    "                'image': img_path,\n",
    "                'bbox': [bbox['xmin'], bbox['ymin'], bbox['xmax'], bbox['ymax']],\n",
    "                'label': class_to_idx[name],\n",
    "                'json_stem': j.stem\n",
    "            })\n",
    "    \n",
    "    print(f\"Dataset: train={len(splits['train'])}, val={len(splits['val'])}, test={len(splits['test'])}\")\n",
    "    return splits, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66036f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO datasets\n",
    "def prepare_yolo_dataset(splits, classes):\n",
    "    import yaml\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        (DATASET_YOLO / 'images' / split).mkdir(parents=True, exist_ok=True)\n",
    "        (DATASET_YOLO / 'labels' / split).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for split, items in splits.items():\n",
    "        for item in tqdm(items, desc=f\"YOLO {split}\"):\n",
    "            with open(item['image'], 'rb') as f:\n",
    "                img = cv2.imdecode(np.frombuffer(f.read(), np.uint8), cv2.IMREAD_COLOR)\n",
    "            h, w = img.shape[:2]\n",
    "            \n",
    "            img_save = DATASET_YOLO / 'images' / split / f\"{item['json_stem']}.jpg\"\n",
    "            cv2.imwrite(str(img_save), img)\n",
    "            \n",
    "            bbox = item['bbox']\n",
    "            x_center = (bbox[0] + bbox[2]) / 2 / w\n",
    "            y_center = (bbox[1] + bbox[3]) / 2 / h\n",
    "            width = (bbox[2] - bbox[0]) / w\n",
    "            height = (bbox[3] - bbox[1]) / h\n",
    "            \n",
    "            label_save = DATASET_YOLO / 'labels' / split / f\"{item['json_stem']}.txt\"\n",
    "            with open(label_save, 'w') as f:\n",
    "                f.write(f\"{item['label']} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "    \n",
    "    # data.yaml\n",
    "    with open(DATASET_YOLO / 'data.yaml', 'w', encoding='utf-8') as f:\n",
    "        yaml.dump({\n",
    "            'path': str(DATASET_YOLO),\n",
    "            'train': 'images/train',\n",
    "            'val': 'images/val',\n",
    "            'test': 'images/test',\n",
    "            'nc': len(classes),\n",
    "            'names': classes\n",
    "        }, f, allow_unicode=True)\n",
    "    \n",
    "    print(f\" YOLO Îç∞Ïù¥ÌÑ∞ÏÖã Ï§ÄÎπÑ ÏôÑÎ£å: {DATASET_YOLO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6c08b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientDet Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "class EffDetDataset(Dataset):\n",
    "    def __init__(self, data, img_size=512):\n",
    "        self.data = data\n",
    "        self.img_size = img_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        with open(item['image'], 'rb') as f:\n",
    "            img = cv2.imdecode(np.frombuffer(f.read(), np.uint8), cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        img = cv2.resize(img, (self.img_size, self.img_size))\n",
    "        img_tensor = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "        \n",
    "        bbox = item['bbox']\n",
    "        bbox_scaled = [\n",
    "            bbox[0] * self.img_size / w,\n",
    "            bbox[1] * self.img_size / h,\n",
    "            bbox[2] * self.img_size / w,\n",
    "            bbox[3] * self.img_size / h\n",
    "        ]\n",
    "        \n",
    "        return img_tensor, {\n",
    "            'bbox': torch.tensor([bbox_scaled], dtype=torch.float32),\n",
    "            'cls': torch.tensor([item['label']], dtype=torch.long),\n",
    "            'img_scale': torch.tensor([1.0], dtype=torch.float32),\n",
    "            'img_size': torch.tensor([self.img_size, self.img_size], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afe83f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = torch.stack([x[0] for x in batch])\n",
    "    max_boxes = max([x[1]['bbox'].shape[0] for x in batch])\n",
    "    \n",
    "    bboxes, classes, scales, sizes = [], [], [], []\n",
    "    for x in batch:\n",
    "        bbox, cls = x[1]['bbox'], x[1]['cls']\n",
    "        n = bbox.shape[0]\n",
    "        if n < max_boxes:\n",
    "            bbox = torch.cat([bbox, torch.zeros((max_boxes - n, 4))])\n",
    "            cls = torch.cat([cls, torch.ones(max_boxes - n, dtype=torch.long) * -1])\n",
    "        bboxes.append(bbox)\n",
    "        classes.append(cls)\n",
    "        scales.append(x[1]['img_scale'])\n",
    "        sizes.append(x[1]['img_size'])\n",
    "    \n",
    "    return images, {\n",
    "        'bbox': torch.stack(bboxes),\n",
    "        'cls': torch.stack(classes),\n",
    "        'img_scale': torch.stack(scales),\n",
    "        'img_size': torch.stack(sizes)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1f0e1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yolov5 Î™®Îç∏\n",
    "def train_yolo(data_yaml, epochs=15):\n",
    "    print(\"\\n YOLOv5 ÌïôÏäµ ÏãúÏûë\")\n",
    "    model = YOLO(str(YOLO_WEIGHTS_FILE))\n",
    "    \n",
    "    results = model.train(\n",
    "        data=str(data_yaml),\n",
    "        epochs=epochs,\n",
    "        imgsz=640,\n",
    "        batch=16,\n",
    "        name='yolov5_freshness',\n",
    "        device='0' if torch.cuda.is_available() else 'cpu',\n",
    "        patience=10,\n",
    "        workers=0,\n",
    "        project=str(RESULT_DIR)\n",
    "    )\n",
    "    \n",
    "    metrics = model.val(\n",
    "        data=str(data_yaml),\n",
    "        project=str(RESULT_DIR),\n",
    "        name='yolov5_freshness'\n",
    "    )\n",
    "    yolo_metrics = {\n",
    "        'mAP50': metrics.box.map50,\n",
    "        'mAP50_95': metrics.box.map,\n",
    "        'precision': metrics.box.mp,\n",
    "        'recall': metrics.box.mr\n",
    "    }\n",
    "    \n",
    "    print(f\"  YOLOv5 ÌïôÏäµ ÏôÑÎ£å\")\n",
    "    print(f\"  mAP@0.5: {yolo_metrics['mAP50']:.3f}\")\n",
    "    print(f\"  mAP@0.5:0.95: {yolo_metrics['mAP50_95']:.3f}\")\n",
    "    \n",
    "    return model, yolo_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d2d5b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO ÌòïÌÉúÎ≥ÄÌôò\n",
    "def create_coco_annotations(splits, classes):    \n",
    "    coco_categories = []\n",
    "    for idx, name in enumerate(classes): \n",
    "        coco_categories.append({\n",
    "            \"id\": idx, \n",
    "            \"name\": name, \n",
    "            \"supercategory\": \"freshness\"\n",
    "        })\n",
    "\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        \n",
    "        coco_images = []\n",
    "        coco_annotations = []\n",
    "        \n",
    "        coco_data = {\n",
    "            'info': {\n",
    "                \"description\": f\"Custom Dataset - {split} Set\",\n",
    "                \"version\": \"1.0\",\n",
    "                \"year\": 2025,\n",
    "                \"contributor\": \"Contributor\",\n",
    "                \"date_created\": \"2025/11/11\"\n",
    "            },\n",
    "            'licenses': [{\"id\": 0, \"name\": \"Unknown\", \"url\": \"\"}],\n",
    "            'categories': coco_categories,\n",
    "            'images': coco_images,\n",
    "            'annotations': coco_annotations\n",
    "        }\n",
    "        \n",
    "        ann_id = 0\n",
    "        \n",
    "        for img_id, item in enumerate(splits[split]):\n",
    "            try:\n",
    "                with open(str(item['image']), 'rb') as f:\n",
    "                    img = cv2.imdecode(np.frombuffer(f.read(), np.uint8), cv2.IMREAD_COLOR)\n",
    "                h, w = img.shape[:2]\n",
    "            except Exception as e:\n",
    "                print(f\"Ïù¥ÎØ∏ÏßÄ Î°úÎìú Ïã§Ìå® {item['image']}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            coco_data['images'].append({\n",
    "                'id': img_id,\n",
    "                'file_name': str(item['image']),\n",
    "                'width': w,\n",
    "                'height': h\n",
    "            })\n",
    "            \n",
    "            bbox = item['bbox']\n",
    "            coco_data['annotations'].append({\n",
    "                'id': ann_id,\n",
    "                'image_id': img_id,\n",
    "                'category_id': item['label'],\n",
    "                'bbox': [bbox[0], bbox[1], bbox[2] - bbox[0], bbox[3] - bbox[1]],\n",
    "                'area': (bbox[2] - bbox[0]) * (bbox[3] - bbox[1]),\n",
    "                'iscrowd': 0\n",
    "            })\n",
    "            ann_id += 1\n",
    "        \n",
    "        anno_path = DATASET_EFFDET / f'coco_{split}.json'\n",
    "        with open(anno_path, 'w') as f:\n",
    "            json.dump(coco_data, f, indent=4)\n",
    "        \n",
    "        print(f\"COCO {split} annotations: {anno_path}\")\n",
    "    \n",
    "    return DATASET_EFFDET / 'coco_test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b584c774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficientdet Î™®Îç∏\n",
    "def train_efficientdet(splits, classes, epochs=50):\n",
    "    print(\"\\n EfficientDet ÌïôÏäµ ÏãúÏûë\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    num_classes = len(classes)\n",
    "    \n",
    "    gt_anno_file = create_coco_annotations(splits, classes)\n",
    "\n",
    "    train_loader = DataLoader(EffDetDataset(splits['train']), batch_size=4,\n",
    "                             shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "    val_loader = DataLoader(EffDetDataset(splits['val']), batch_size=4,\n",
    "                           collate_fn=collate_fn, num_workers=0)\n",
    "    \n",
    "    config = get_efficientdet_config('tf_efficientdet_d0')\n",
    "    config.num_classes = num_classes\n",
    "    config.image_size = (512, 512)\n",
    "    \n",
    "    model = DetBenchTrain(EfficientDet(config, pretrained_backbone=True), config)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    max_patience = 10\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    val_recalls = []\n",
    "    val_maps = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # ÌïôÏäµ\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            images = images.to(device)\n",
    "            targets = {k: v.to(device) for k, v in targets.items()}\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(images, targets)\n",
    "            loss = output['loss'] if isinstance(output, dict) else output\n",
    "            \n",
    "            # NaN Ï≤¥ÌÅ¨\n",
    "            if torch.isnan(loss):\n",
    "                print(\" NaN loss detected\")\n",
    "                continue\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Í≤ÄÏ¶ù\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images = images.to(device)\n",
    "                targets = {k: v.to(device) for k, v in targets.items()}\n",
    "                output = model(images, targets)\n",
    "                loss = output['loss'] if isinstance(output, dict) else output\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss) \n",
    "        print(f\"Epoch {epoch+1}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch == 0 or val_loss < best_loss:\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "            \n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.model.state_dict(),\n",
    "                'config': config\n",
    "            }, RESULT_DIR / 'efficientdet_best.pth')\n",
    "            print(f\" Saved (Loss: {best_loss:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= max_patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        scheduler.step()\n",
    "        effdet_metrics = evaluate_efficientdet_coco(model, config, splits, classes, device, gt_anno_file)\n",
    "        val_maps.append(effdet_metrics['mAP50_95'])\n",
    "        val_recalls.append(effdet_metrics['recall'])    \n",
    "    print(f\" EfficientDet ÌïôÏäµ ÏôÑÎ£å (Best Loss: {best_loss:.4f})\")\n",
    "\n",
    "    #loss\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title(\"EfficientDet Training Loss Curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULT_DIR / \"efficientdet_loss_curve.png\")\n",
    "    plt.close()\n",
    "    print(f\"EfficientDet ÌïôÏäµ Í≥°ÏÑ† Ï†ÄÏû•Îê®: {RESULT_DIR / 'efficientdet_loss_curve.png'}\")\n",
    "          \n",
    "    # mAP/recall\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(val_maps, label='Validation mAP50_95')\n",
    "    plt.plot(val_recalls, label='Validation Recall')\n",
    "    plt.title(\"EfficientDet Validation mAP/Recall Curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULT_DIR / \"efficientdet_map_curve.png\")\n",
    "    plt.close()\n",
    "    print(f\"EfficientDet mAP/Recall Í≥°ÏÑ† Ï†ÄÏû•Îê®: {RESULT_DIR / 'efficientdet_map_curve.png'}\")\n",
    "\n",
    "    return model, effdet_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18b44e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO ÌèâÍ∞ÄÏßÄÌëú\n",
    "def evaluate_efficientdet_coco(model, config, splits, classes, device, gt_anno_file):\n",
    "    print(\"\\n EfficientDet ÌèâÍ∞Ä Ï§ë (COCO API)\")\n",
    "    \n",
    "    if not COCO_AVAILABLE:\n",
    "        print(\" COCO API ÏóÜÏùå.\")\n",
    "        return evaluate_efficientdet_simple(None, config, splits, device)\n",
    "\n",
    "    checkpoint = torch.load(RESULT_DIR / 'efficientdet_best.pth', weights_only=False)\n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    from effdet import DetBenchPredict\n",
    "    bench = DetBenchPredict(net)\n",
    "    bench.eval()\n",
    "    bench.to(device)\n",
    "    \n",
    "    coco_gt = COCO(str(gt_anno_file))\n",
    "    \n",
    "    results = []\n",
    "    test_data = splits['test']\n",
    "    \n",
    "    vis_dir = RESULT_DIR / 'efficientdet_predictions'\n",
    "    vis_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # ÎîîÎ≤ÑÍπÖ\n",
    "    total_detections = 0\n",
    "    detection_scores = []\n",
    "    debug_output = True\n",
    "    \n",
    "    for img_id, item in enumerate(tqdm(test_data, desc=\"Predicting\")):\n",
    "        with open(item['image'], 'rb') as f:\n",
    "            img = cv2.imdecode(np.frombuffer(f.read(), np.uint8), cv2.IMREAD_COLOR)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img_rgb.shape[:2]\n",
    "        \n",
    "        # Ï∂îÎ°†\n",
    "        img_resized = cv2.resize(img_rgb, (512, 512))\n",
    "        img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).float() / 255.0\n",
    "        img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            detections = bench(img_tensor)\n",
    "            \n",
    "            # ÎîîÎ≤ÑÍπÖ\n",
    "            if debug_output:\n",
    "                print(f\"\\n[DEBUG] DetBenchPredict Ï∂úÎ†• ÌÉÄÏûÖ: {type(detections)}\")\n",
    "                if hasattr(detections, 'shape'):\n",
    "                    print(f\"[DEBUG] Ï∂úÎ†• shape: {detections.shape}\")\n",
    "                    if detections.shape[0] > 0:\n",
    "                        print(f\"[DEBUG] ÏÉòÌîå (Ï≤´ 5Í∞ú):\\n{detections[0][:5]}\")\n",
    "                else:\n",
    "                    print(f\"[DEBUG] ÏòàÏÉÅÏπò Î™ªÌïú Ï∂úÎ†•\")\n",
    "                debug_output = False\n",
    "        \n",
    "        vis_img = img.copy()\n",
    "        \n",
    "        conf_thresh = 0.001\n",
    "        \n",
    "        if detections is not None and hasattr(detections, 'shape') and len(detections.shape) == 3:\n",
    "            det = detections[0].cpu()  \n",
    "            \n",
    "            for i in range(det.shape[0]):\n",
    "                if det.shape[1] >= 6:\n",
    "                    score = float(det[i, 4])\n",
    "                    class_id = int(det[i, 5])\n",
    "                elif det.shape[1] == 5:\n",
    "                    score = float(det[i, 4])\n",
    "                    class_id = 0  \n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                if score < conf_thresh:\n",
    "                    continue\n",
    "                \n",
    "                detection_scores.append(score)\n",
    "                total_detections += 1\n",
    "                \n",
    "                x1 = float(det[i, 0]) * w / 512\n",
    "                y1 = float(det[i, 1]) * h / 512\n",
    "                x2 = float(det[i, 2]) * w / 512\n",
    "                y2 = float(det[i, 3]) * h / 512\n",
    "                \n",
    "                if class_id < 0 or class_id >= len(classes):\n",
    "                    continue\n",
    "                if x2 <= x1 or y2 <= y1:\n",
    "                    continue\n",
    "                \n",
    "                results.append({\n",
    "                    'image_id': img_id,\n",
    "                    'category_id': class_id,\n",
    "                    'bbox': [x1, y1, x2 - x1, y2 - y1],\n",
    "                    'score': score\n",
    "                })\n",
    "            \n",
    "                if score > 0.01:\n",
    "\n",
    "                    # Ï¢åÌëú ÌÅ¥Î¶¨Ìïë Ï∂îÍ∞Ä\n",
    "                    x1_clipped = max(0, min(int(x1), w))\n",
    "                    y1_clipped = max(0, min(int(y1), h))\n",
    "                    x2_clipped = max(0, min(int(x2), w))\n",
    "                    y2_clipped = max(0, min(int(y2), h))\n",
    "                    \n",
    "                    # Ïú†Ìö®Ìïú bboxÏù∏ÏßÄ ÌôïÏù∏\n",
    "                    if x2_clipped > x1_clipped and y2_clipped > y1_clipped:\n",
    "                        cv2.rectangle(vis_img, (x1_clipped, y1_clipped), (x2_clipped, y2_clipped), (0, 255, 0), 2)\n",
    "                        label = f\"{classes[class_id]}: {score:.2f}\"\n",
    "                        org = (x1_clipped, max(10, y1_clipped - 10))\n",
    "                        cv2.putText(vis_img, str(label), org, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2)\n",
    "            \n",
    "            # ÏòàÏ∏° Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "            cv2.imwrite(str(vis_dir / f\"{item['json_stem']}_pred.jpg\"), vis_img)\n",
    "    \n",
    "    print(f\"\\n[DEBUG] Ï¥ù detections: {total_detections}Í∞ú\")\n",
    "    if detection_scores:\n",
    "        print(f\"[DEBUG] Score Î≤îÏúÑ: {min(detection_scores):.4f} ~ {max(detection_scores):.4f}\")\n",
    "        print(f\"[DEBUG] Score ÌèâÍ∑†: {np.mean(detection_scores):.4f}\")\n",
    "    print(f\"[DEBUG] ÏµúÏ¢Ö ÏòàÏ∏° Í≤∞Í≥º: {len(results)}Í∞ú\")\n",
    "    \n",
    "    if not results:\n",
    "        print(\"ÏòàÏ∏° Í≤∞Í≥º ÏóÜÏùå!\")\n",
    "        print(\"Î™®Îç∏Ïù¥ ÏïÑÎ¨¥Í≤ÉÎèÑ Í≤ÄÏ∂úÌïòÏßÄ Î™ªÌñàÏäµÎãàÎã§.\")\n",
    "        print(\"Ìï¥Í≤∞ Î∞©Î≤ï:\")\n",
    "        print(\"1) epochsÎ•º 100 Ïù¥ÏÉÅÏúºÎ°ú Ï¶ùÍ∞Ä\")\n",
    "        print(\"2) learning rate 0.01Î°ú Ï¶ùÍ∞Ä\")\n",
    "        print(\"3) Îç∞Ïù¥ÌÑ∞ÏÖãÏù¥ ÎÑàÎ¨¥ ÏûëÏùÄÏßÄ ÌôïÏù∏ (ÏµúÏÜå 100Ïû• Ïù¥ÏÉÅ Í∂åÏû•)\")\n",
    "        print(\"4) ÌÅ¥ÎûòÏä§ Í∞úÏàò ÌôïÏù∏\")\n",
    "        return evaluate_efficientdet_simple(checkpoint, config, splits, device)\n",
    "    \n",
    "    # ÏòàÏ∏° Í≤∞Í≥º Ï†ÄÏû•\n",
    "    pred_file = RESULT_DIR / 'coco_predictions.json'\n",
    "    with open(pred_file, 'w') as f:\n",
    "        json.dump(results, f)\n",
    "    \n",
    "    # COCO ÌèâÍ∞Ä\n",
    "    try:\n",
    "        coco_dt = coco_gt.loadRes(str(pred_file))\n",
    "        coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "        coco_eval.evaluate()\n",
    "        coco_eval.accumulate()\n",
    "        coco_eval.summarize()\n",
    "        \n",
    "        metrics = {\n",
    "            'mAP50_95': coco_eval.stats[0],\n",
    "            'mAP50': coco_eval.stats[1],\n",
    "            'mAP75': coco_eval.stats[2],\n",
    "            'precision': coco_eval.stats[0],\n",
    "            'recall': coco_eval.stats[8]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"COCO ÌèâÍ∞Ä Ïò§Î•ò: {e}\")\n",
    "        return evaluate_efficientdet_simple(checkpoint, config, splits, device)\n",
    "    \n",
    "    print(f\" EfficientDet ÌèâÍ∞Ä ÏôÑÎ£å (COCO API)\")\n",
    "    print(f\"  mAP@0.5: {metrics['mAP50']:.3f}\")\n",
    "    print(f\"  mAP@0.5:0.95: {metrics['mAP50_95']:.3f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e6e2342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_efficientdet_simple(model, config, splits, device):\n",
    "    print(\"\\n EfficientDet ÌèâÍ∞Ä Ï§ë\")\n",
    "    \n",
    "    checkpoint = torch.load(RESULT_DIR / 'efficientdet_best.pth', weights_only=False)\n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    predictor = DetBenchPredict(net).to(device).eval()\n",
    "    \n",
    "    test_data = splits['test']\n",
    "    \n",
    "    def calculate_iou(box1, box2):\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[2], box2[2])\n",
    "        y2 = min(box1[3], box2[3])\n",
    "        \n",
    "        inter = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        union = area1 + area2 - inter\n",
    "        \n",
    "        return inter / union if union > 0 else 0\n",
    "    \n",
    "    correct_50 = 0\n",
    "    all_ious = []\n",
    "    \n",
    "    for item in tqdm(test_data, desc=\"Evaluating\"):\n",
    "        with open(item['image'], 'rb') as f:\n",
    "            img = cv2.imdecode(np.frombuffer(f.read(), np.uint8), cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        img_resized = cv2.resize(img, (512, 512))\n",
    "        img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).float() / 255.0\n",
    "        img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = predictor(img_tensor)\n",
    "        \n",
    "        gt_box = item['bbox']\n",
    "        gt_label = item['label']\n",
    "        \n",
    "        best_iou = 0\n",
    "        \n",
    "        if len(pred) > 0 and pred[0].shape[0] > 0:\n",
    "            pred_np = pred[0].cpu().numpy()\n",
    "            for box in pred_np:\n",
    "                if box[4] < 0.1:\n",
    "                    continue\n",
    "                \n",
    "                pred_box = [\n",
    "                    box[0] * w / 512, box[1] * h / 512,\n",
    "                    box[2] * w / 512, box[3] * h / 512\n",
    "                ]\n",
    "                pred_label = int(box[5]) if len(box) > 5 else int(box[4])\n",
    "                \n",
    "                if pred_label == gt_label:\n",
    "                    iou = calculate_iou(gt_box, pred_box)\n",
    "                    best_iou = max(best_iou, iou)\n",
    "        \n",
    "        all_ious.append(best_iou)\n",
    "        if best_iou >= 0.5:\n",
    "            correct_50 += 1\n",
    "    \n",
    "    total = len(test_data)\n",
    "    mAP_50 = correct_50 / total if total > 0 else 0\n",
    "    \n",
    "    mAP_50_95_sum = 0\n",
    "    for thresh in [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]:\n",
    "        correct = sum([1 for iou in all_ious if iou >= thresh])\n",
    "        mAP_50_95_sum += (correct / total if total > 0 else 0)\n",
    "    mAP_50_95 = mAP_50_95_sum / 10\n",
    "    \n",
    "    metrics = {\n",
    "        'mAP50': mAP_50,\n",
    "        'mAP50_95': mAP_50_95,\n",
    "        'precision': mAP_50,\n",
    "        'recall': mAP_50\n",
    "    }\n",
    "    \n",
    "    print(f\"  EfficientDet ÌèâÍ∞Ä ÏôÑÎ£å\")\n",
    "    print(f\"  mAP@0.5: {metrics['mAP50']:.3f}\")\n",
    "    print(f\"  mAP@0.5:0.95: {metrics['mAP50_95']:.3f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f60adc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏãúÍ∞ÅÌôî\n",
    "def visualize_comparison(yolo_metrics, effdet_metrics):\n",
    "    metrics_names = ['mAP@0.5', 'mAP@0.5:0.95', 'Precision', 'Recall']\n",
    "    yolo_values = [\n",
    "        yolo_metrics['mAP50'],\n",
    "        yolo_metrics['mAP50_95'],\n",
    "        yolo_metrics['precision'],\n",
    "        yolo_metrics['recall']\n",
    "    ]\n",
    "    effdet_values = [\n",
    "        effdet_metrics['mAP50'],\n",
    "        effdet_metrics['mAP50_95'],\n",
    "        effdet_metrics['precision'],\n",
    "        effdet_metrics['recall']\n",
    "    ]\n",
    "    \n",
    "    x = np.arange(len(metrics_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    bars1 = ax.bar(x - width/2, yolo_values, width, label='YOLOv5', color='red')\n",
    "    bars2 = ax.bar(x + width/2, effdet_values, width, label='EfficientDet', color='blue')\n",
    "    \n",
    "    ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('YOLOv5 vs EfficientDet Performance Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics_names)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Í∞í ÌëúÏãú\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}',\n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULT_DIR / 'performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n ÎπÑÍµê Í∑∏ÎûòÌîÑ Ï†ÄÏû•: {RESULT_DIR / 'performance_comparison.png'}\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165f1a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "YOLOv5 vs EfficientDet ÏÑ±Îä• ÎπÑÍµê\n",
      "============================================================\n",
      "\n",
      "1. Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ Ï§ë...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 72/72 [00:00<00:00, 7416.44it/s]\n",
      "Loading val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 4335.45it/s]\n",
      "Loading test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 5260.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: train=72, val=9, test=9\n",
      "ÌÅ¥ÎûòÏä§ (9Í∞ú): ['ÏÇ¨Í≥º_Ìäπ', 'Í∞ê_ÏÉÅ', 'ÏÇ¨Í≥º_Î≥¥ÌÜµ', 'Í∞ê_Ìäπ', 'ÏÇ¨Í≥º_ÏÉÅ', 'Î∞∞_Î≥¥ÌÜµ', 'Í∞ê_Î≥¥ÌÜµ', 'Î∞∞_Ìäπ', 'Î∞∞_ÏÉÅ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "YOLO train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 72/72 [00:01<00:00, 53.03it/s]\n",
      "YOLO val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 54.51it/s]\n",
      "YOLO test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 58.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " YOLO Îç∞Ïù¥ÌÑ∞ÏÖã Ï§ÄÎπÑ ÏôÑÎ£å: /Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/preprocessed_data/yolov5\n",
      "\n",
      " YOLOv5 ÌïôÏäµ ÏãúÏûë\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov5su.pt to '/Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/results_comparison/yolov5su.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 17.7MB 29.8MB/s 0.6s0.5s<0.1s\n",
      "New https://pypi.org/project/ultralytics/8.3.227 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.225 üöÄ Python-3.10.19 torch-2.9.0 CPU (Apple M1)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/preprocessed_data/yolov5/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=1, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=/Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/results_comparison/yolov5su.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov5_freshness, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/results_comparison, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/results_comparison/yolov5_freshness, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=0, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=9\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      3520  ultralytics.nn.modules.conv.Conv             [3, 32, 6, 2, 2]              \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     18816  ultralytics.nn.modules.block.C3              [64, 64, 1]                   \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    115712  ultralytics.nn.modules.block.C3              [128, 128, 2]                 \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  3    625152  ultralytics.nn.modules.block.C3              [256, 256, 3]                 \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1]                 \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1]              \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    361984  ultralytics.nn.modules.block.C3              [512, 256, 1, False]          \n",
      " 14                  -1  1     33024  ultralytics.nn.modules.conv.Conv             [256, 128, 1, 1]              \n",
      " 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 16             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1     90880  ultralytics.nn.modules.block.C3              [256, 128, 1, False]          \n",
      " 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 19            [-1, 14]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1    296448  ultralytics.nn.modules.block.C3              [256, 256, 1, False]          \n",
      " 21                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 22            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 23                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1, False]          \n",
      " 24        [17, 20, 23]  1   2119531  ultralytics.nn.modules.head.Detect           [9, [128, 256, 512]]          \n",
      "YOLOv5s summary: 153 layers, 9,125,675 parameters, 9,125,659 gradients, 24.1 GFLOPs\n",
      "\n",
      "Transferred 421/427 items from pretrained weights\n",
      "Freezing layer 'model.24.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 5265.7¬±1761.1 MB/s, size: 184.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/preprocessed_data/yolov5/labels/train... 72 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 4.2Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/preprocessed_data/yolov5/labels/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 5985.9¬±1941.4 MB/s, size: 216.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/preprocessed_data/yolov5/labels/val... 9 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 4.6Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/preprocessed_data/yolov5/labels/val.cache\n",
      "Plotting labels to /Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/results_comparison/yolov5_freshness/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000769, momentum=0.9) with parameter groups 69 weight(decay=0.0), 76 weight(decay=0.0005), 75 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/results_comparison/yolov5_freshness\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        1/1         0G      1.314      4.038      1.975         29        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 0.1it/s 54.0s7.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 0.6it/s 1.7s\n",
      "                   all          9          9      0.393      0.375      0.435        0.3\n",
      "\n",
      "1 epochs completed in 0.016 hours.\n",
      "Optimizer stripped from /Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/results_comparison/yolov5_freshness/weights/last.pt, 18.5MB\n",
      "Optimizer stripped from /Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/results_comparison/yolov5_freshness/weights/best.pt, 18.5MB\n",
      "\n",
      "Validating /Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/results_comparison/yolov5_freshness/weights/best.pt...\n",
      "Ultralytics 8.3.225 üöÄ Python-3.10.19 torch-2.9.0 CPU (Apple M1)\n",
      "YOLOv5s summary (fused): 84 layers, 9,115,019 parameters, 0 gradients, 23.8 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 0.6it/s 1.6s\n",
      "                   all          9          9      0.393      0.375      0.439      0.304\n",
      "                  ÏÇ¨Í≥º_Ìäπ          1          1          0          0      0.199      0.179\n",
      "                 ÏÇ¨Í≥º_Î≥¥ÌÜµ          1          1          1          0          0          0\n",
      "                   Í∞ê_Ìäπ          1          1      0.143          1      0.995      0.895\n",
      "                  ÏÇ¨Í≥º_ÏÉÅ          1          1          0          0     0.0765     0.0765\n",
      "                  Î∞∞_Î≥¥ÌÜµ          1          1      0.857          1      0.995      0.796\n",
      "                  Í∞ê_Î≥¥ÌÜµ          2          2      0.143          1      0.235      0.155\n",
      "                   Î∞∞_Ìäπ          1          1          0          0     0.0184    0.00921\n",
      "                   Î∞∞_ÏÉÅ          1          1          1          0      0.995      0.318\n",
      "Speed: 0.8ms preprocess, 161.1ms inference, 0.0ms loss, 8.8ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/results_comparison/yolov5_freshness\u001b[0m\n",
      "Ultralytics 8.3.225 üöÄ Python-3.10.19 torch-2.9.0 CPU (Apple M1)\n",
      "YOLOv5s summary (fused): 84 layers, 9,115,019 parameters, 0 gradients, 23.8 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 5294.2¬±721.9 MB/s, size: 201.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/preprocessed_data/yolov5/labels/val.cache... 9 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 20.5Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 0.7it/s 1.4s\n",
      "                   all          9          9      0.393      0.375      0.439      0.304\n",
      "                  ÏÇ¨Í≥º_Ìäπ          1          1          0          0      0.199      0.179\n",
      "                 ÏÇ¨Í≥º_Î≥¥ÌÜµ          1          1          1          0          0          0\n",
      "                   Í∞ê_Ìäπ          1          1      0.143          1      0.995      0.895\n",
      "                  ÏÇ¨Í≥º_ÏÉÅ          1          1          0          0     0.0765     0.0765\n",
      "                  Î∞∞_Î≥¥ÌÜµ          1          1      0.857          1      0.995      0.796\n",
      "                  Í∞ê_Î≥¥ÌÜµ          2          2      0.143          1      0.235      0.155\n",
      "                   Î∞∞_Ìäπ          1          1          0          0     0.0184    0.00921\n",
      "                   Î∞∞_ÏÉÅ          1          1          1          0      0.995      0.318\n",
      "Speed: 0.7ms preprocess, 139.9ms inference, 0.0ms loss, 8.9ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/results_comparison/yolov5_freshness2\u001b[0m\n",
      "  YOLOv5 ÌïôÏäµ ÏôÑÎ£å\n",
      "  mAP@0.5: 0.439\n",
      "  mAP@0.5:0.95: 0.304\n",
      "\n",
      " EfficientDet ÌïôÏäµ ÏãúÏûë\n",
      "COCO train annotations: /Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/preprocessed_data/efficientdet/coco_train.json\n",
      "COCO val annotations: /Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/preprocessed_data/efficientdet/coco_val.json\n",
      "COCO test annotations: /Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/preprocessed_data/efficientdet/coco_test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
      "Epoch 1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [01:05<00:00,  3.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train=2.0209, Val=472.1954\n",
      " Saved (Loss: 472.1954)\n",
      "\n",
      " EfficientDet ÌèâÍ∞Ä Ï§ë (COCO API)\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  11%|‚ñà         | 1/9 [00:00<00:03,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] DetBenchPredict Ï∂úÎ†• ÌÉÄÏûÖ: <class 'torch.Tensor'>\n",
      "[DEBUG] Ï∂úÎ†• shape: torch.Size([1, 100, 6])\n",
      "[DEBUG] ÏÉòÌîå (Ï≤´ 5Í∞ú):\n",
      "tensor([[-136.1457, -457.7244, -135.9995, -457.7231,    1.0000,    4.0000],\n",
      "        [-142.5868, -453.7272, -142.4715, -453.7257,    1.0000,    4.0000],\n",
      "        [-129.1400, -421.6944, -128.9115, -421.6935,    1.0000,    4.0000],\n",
      "        [-141.4227, -440.2444, -141.3134, -440.2423,    1.0000,    4.0000],\n",
      "        [-125.1345, -414.1893, -124.8294, -414.1885,    1.0000,    4.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] Ï¥ù detections: 900Í∞ú\n",
      "[DEBUG] Score Î≤îÏúÑ: 0.9996 ~ 1.0000\n",
      "[DEBUG] Score ÌèâÍ∑†: 1.0000\n",
      "[DEBUG] ÏµúÏ¢Ö ÏòàÏ∏° Í≤∞Í≥º: 319Í∞ú\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.00s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " EfficientDet ÌèâÍ∞Ä ÏôÑÎ£å (COCO API)\n",
      "  mAP@0.5: 0.000\n",
      "  mAP@0.5:0.95: 0.000\n",
      " EfficientDet ÌïôÏäµ ÏôÑÎ£å (Best Loss: 472.1954)\n",
      "EfficientDet ÌïôÏäµ Í≥°ÏÑ† Ï†ÄÏû•Îê®: /Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/results_comparison/efficientdet_loss_curve.png\n",
      "EfficientDet mAP/Recall Í≥°ÏÑ† Ï†ÄÏû•Îê®: /Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/results_comparison/efficientdet_map_curve.png\n",
      "\n",
      "============================================================\n",
      "ÏµúÏ¢Ö ÏÑ±Îä• ÎπÑÍµê\n",
      "============================================================\n",
      "\n",
      "Metric               YOLOv5          EfficientDet    Difference     \n",
      "-----------------------------------------------------------------\n",
      "mAP@0.5              0.439           0.000           +0.439\n",
      "mAP@0.5:0.95         0.304           0.000           +0.304\n",
      "Precision            0.393           0.000           +0.393\n",
      "Recall               0.375           0.000           +0.375\n",
      "\n",
      "=================================================================\n",
      "üèÜ YOLOv5Í∞Ä 0.439ÎßåÌÅº Îçî ÎÜíÏùÄ mAP@0.5Î•º Îã¨ÏÑ±ÌñàÏäµÎãàÎã§!\n",
      "=================================================================\n",
      "\n",
      " ÎπÑÍµê Í∑∏ÎûòÌîÑ Ï†ÄÏû•: /Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/results_comparison/performance_comparison.png\n",
      "\n",
      " Í≤∞Í≥º Ï†ÄÏû• ÏúÑÏπò:\n",
      "  - Ï†ÑÏ≤¥ Í≤∞Í≥º: /Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/results_comparison\n",
      "  - YOLOv5 ÌïôÏäµ: /Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/results_comparison/yolov5_freshness\n",
      "  - EfficientDet ÏòàÏ∏°: /Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/results_comparison/efficientdet_predictions\n",
      "  - ÎπÑÍµê Í∑∏ÎûòÌîÑ: /Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/results_comparison/performance_comparison.png\n",
      "  - Í≤∞Í≥º ÏöîÏïΩ: /Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/results_comparison/comparison_results.json\n",
      " ÏÑ±Îä• ÎπÑÍµê Í∑∏ÎûòÌîÑ Ï†ÄÏû•Îê®: /Users/handaeseong/dev/data-engineer/mini-project-2-fruits/processed/results_comparison/comparison_graph.png\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"YOLOv5 vs EfficientDet ÏÑ±Îä• ÎπÑÍµê\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨\n",
    "    print(\"\\n1. Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ Ï§ë...\")\n",
    "    splits, classes = preprocess_data()\n",
    "    if not classes:\n",
    "        print(\"Îç∞Ïù¥ÌÑ∞ÏÖã ÏóÜÏùå\")\n",
    "        return\n",
    "    print(f\"ÌÅ¥ÎûòÏä§ ({len(classes)}Í∞ú): {classes}\")\n",
    "    \n",
    "    # 2. YOLOv5\n",
    "    prepare_yolo_dataset(splits, classes)\n",
    "    yolo_model, yolo_metrics = train_yolo(DATASET_YOLO / 'data.yaml', epochs=10)\n",
    "\n",
    "    # 3. EfficientDet\n",
    "    effdet_model, effdet_metrics = train_efficientdet(splits, classes, epochs=50)\n",
    "    \n",
    "    # 4. Í≤∞Í≥º ÎπÑÍµê\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ÏµúÏ¢Ö ÏÑ±Îä• ÎπÑÍµê\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n{'Metric':<20} {'YOLOv5':<15} {'EfficientDet':<15} {'Difference':<15}\")\n",
    "    print(\"-\"*65)\n",
    "    print(f\"{'mAP@0.5':<20} {yolo_metrics['mAP50']:<15.3f} {effdet_metrics['mAP50']:<15.3f} {yolo_metrics['mAP50']-effdet_metrics['mAP50']:+.3f}\")\n",
    "    print(f\"{'mAP@0.5:0.95':<20} {yolo_metrics['mAP50_95']:<15.3f} {effdet_metrics['mAP50_95']:<15.3f} {yolo_metrics['mAP50_95']-effdet_metrics['mAP50_95']:+.3f}\")\n",
    "    print(f\"{'Precision':<20} {yolo_metrics['precision']:<15.3f} {effdet_metrics['precision']:<15.3f} {yolo_metrics['precision']-effdet_metrics['precision']:+.3f}\")\n",
    "    print(f\"{'Recall':<20} {yolo_metrics['recall']:<15.3f} {effdet_metrics['recall']:<15.3f} {yolo_metrics['recall']-effdet_metrics['recall']:+.3f}\")\n",
    "    \n",
    "    # ÏäπÏûê Í≤∞Ï†ï\n",
    "    if yolo_metrics['mAP50'] > effdet_metrics['mAP50']:\n",
    "        winner = \"YOLOv5\"\n",
    "        diff = yolo_metrics['mAP50'] - effdet_metrics['mAP50']\n",
    "    elif effdet_metrics['mAP50'] > yolo_metrics['mAP50']:\n",
    "        winner = \"EfficientDet\"\n",
    "        diff = effdet_metrics['mAP50'] - yolo_metrics['mAP50']\n",
    "    else:\n",
    "        winner = \"ÎèôÏ†ê\"\n",
    "        diff = 0\n",
    "    \n",
    "    print(f\"\\n{'='*65}\")\n",
    "    if winner != \"ÎèôÏ†ê\":\n",
    "        print(f\"üèÜ {winner}Í∞Ä {diff:.3f}ÎßåÌÅº Îçî ÎÜíÏùÄ mAP@0.5Î•º Îã¨ÏÑ±ÌñàÏäµÎãàÎã§!\")\n",
    "    else:\n",
    "        print(f\"üèÜ Îëê Î™®Îç∏Ïù¥ ÎèôÏùºÌïú ÏÑ±Îä•ÏùÑ Î≥¥ÏòÄÏäµÎãàÎã§!\")\n",
    "    print(f\"{'='*65}\")\n",
    "    \n",
    "    # 5. ÏãúÍ∞ÅÌôî\n",
    "    visualize_comparison(yolo_metrics, effdet_metrics)\n",
    "    \n",
    "    # 6. Í≤∞Í≥º Ï†ÄÏû•\n",
    "    results_summary = {\n",
    "        'yolo': yolo_metrics,\n",
    "        'efficientdet': effdet_metrics,\n",
    "        'winner': winner,\n",
    "        'classes': classes\n",
    "    }\n",
    "    \n",
    "    with open(RESULT_DIR / 'comparison_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(results_summary, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n Í≤∞Í≥º Ï†ÄÏû• ÏúÑÏπò:\")\n",
    "    print(f\"  - Ï†ÑÏ≤¥ Í≤∞Í≥º: {RESULT_DIR}\")\n",
    "    print(f\"  - YOLOv5 ÌïôÏäµ: {RESULT_DIR / 'yolov5_freshness'}\")\n",
    "    print(f\"  - EfficientDet ÏòàÏ∏°: {RESULT_DIR / 'efficientdet_predictions'}\")\n",
    "    print(f\"  - ÎπÑÍµê Í∑∏ÎûòÌîÑ: {RESULT_DIR / 'performance_comparison.png'}\")\n",
    "    print(f\"  - Í≤∞Í≥º ÏöîÏïΩ: {RESULT_DIR / 'comparison_results.json'}\")\n",
    "\n",
    "    # ÏÑ±Îä• ÎπÑÍµê Í∑∏ÎûòÌîÑ\n",
    "    labels = ['mAP@0.5', 'mAP@0.5:0.95']\n",
    "    yolo_scores = [yolo_metrics['mAP50'], yolo_metrics['mAP50_95']]\n",
    "    effdet_scores = [effdet_metrics['mAP50'], effdet_metrics['mAP50_95']]\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(x - width/2, yolo_scores, width, label='YOLOv5')\n",
    "    plt.bar(x + width/2, effdet_scores, width, label='EfficientDet')\n",
    "\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('YOLOv5 vs EfficientDet Performance')\n",
    "    plt.xticks(x, labels)\n",
    "    plt.legend()\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULT_DIR / 'comparison_graph.png')\n",
    "    plt.close()\n",
    "    print(f\" ÏÑ±Îä• ÎπÑÍµê Í∑∏ÎûòÌîÑ Ï†ÄÏû•Îê®: {RESULT_DIR / 'comparison_graph.png'}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d5df4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
